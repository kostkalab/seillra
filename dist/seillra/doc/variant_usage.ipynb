{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc6025c4",
   "metadata": {},
   "source": [
    "Import the seillra package, and other helpful packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cc2e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "import seillra as sl\n",
    "import seimodel as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd356337",
   "metadata": {},
   "source": [
    "To load different parts of the models, select a rank, whether the output is chromatin profiles or sequence classes, whether you are predicting for any sequences or specifically variants, and if the model should be on the CPU or GPU. These are selectable when using the `Sei_LLRA` model class, but other functionallity can be customized using the model blocks explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0499ea7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using a MAC\n",
    "if \"qnnpack\" in torch.backends.quantized.supported_engines:\n",
    "    torch.backends.quantized.engine = \"qnnpack\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25a07b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = 256 # 1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048\n",
    "sequence_classes = False # True\n",
    "sequence_type = \"sequence\" # \"variant\"\n",
    "device = \"cpu\" # \"cuda\", \"cuda:0\"\n",
    "model = sl.Sei_LLRA(k=rank, projection=sequence_classes, mode = sequence_type, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37098f31",
   "metadata": {},
   "source": [
    "This can be done using `torch.nn.Sequential` as well. For an un-quantized (GPU) model see the commented code. Note that one should be careful about handeling forward and reverse-complement sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1563668",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_trunk = sl.get_sei_trunk_q() # sm.get_sei_trunk().load_weights()\n",
    "mod_head  = sl.get_sei_head_llra_q(k=rank) # sl.get_sei_head_llra(k=rank)\n",
    "mod_projection = sm.get_sei_projection().load_weights()\n",
    "mod_projection.set_mode(sequence_type)\n",
    "\n",
    "#- Make a full model\n",
    "mod = torch.nn.Sequential(collections.OrderedDict([\n",
    "    ('trunk', mod_trunk),\n",
    "    ('head', mod_head),\n",
    "    ('projection', mod_projection)\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d344af6",
   "metadata": {},
   "source": [
    "Here is an example of running the model on a random one-hot encoded sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a7dc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = torch.randint(0, 4, (16, 4096))\n",
    "x = torch.nn.functional.one_hot(sequences, num_classes=4).permute(0, 2, 1).float()\n",
    "print(x.shape)\n",
    "# - run the model\n",
    "out = model(x.to(\"cpu\"))\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd4d5a8",
   "metadata": {},
   "source": [
    "Now we can try a different rank, use the sequenc classes and use the GPU. Note that this will remain on the CPU if you do not have access to a cuda enabled GPU (Apple M-series GPUs are not cuda enabled)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e40de23",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = 1 # 1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048\n",
    "sequence_classes = True # True\n",
    "sequence_type = \"sequence\" # \"variant\"\n",
    "device = \"cuda\" # \"cpu\", \"cuda:0\"\n",
    "model2 = sl.Sei_LLRA(k=rank, projection=sequence_classes, mode = sequence_type, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c7ead4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = torch.randint(0, 4, (16, 4096))\n",
    "x = torch.nn.functional.one_hot(sequences, num_classes=4).permute(0, 2, 1).float()\n",
    "print(x.shape)\n",
    "# - run the model\n",
    "out = model2(x.to(\"cuda\"))\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c5207f",
   "metadata": {},
   "source": [
    "This can also be done for getting predictions for variants. We will set up example reference and alternate allele sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101af095",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = 1 # 1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048\n",
    "sequence_classes = True # False\n",
    "sequence_type = \"variant\" # \"sequence\"\n",
    "device = \"cpu\" # \"cuda\", \"cuda:0\"\n",
    "model3 = sl.Sei_LLRA(k=rank, projection=sequence_classes, mode = sequence_type, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac87a6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_sequences = torch.randint(0, 4, (16, 4096))\n",
    "alt_sequences = ref_sequences.clone()\n",
    "center_idx = 4096 // 2\n",
    "alt_sequences[:, center_idx] = (ref_sequences[:, center_idx] + 1) % 4\n",
    "\n",
    "\n",
    "x_ref = torch.nn.functional.one_hot(ref_sequences, num_classes=4).permute(0, 2, 1).float()\n",
    "x_alt = torch.nn.functional.one_hot(alt_sequences, num_classes=4).permute(0, 2, 1).float()\n",
    "\n",
    "print(x_ref.shape, x_alt.shape)\n",
    "input = (x_ref, x_alt)\n",
    "out_ref, out_alt = model3(input)\n",
    "print(out_ref.shape, out_alt.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a90917d",
   "metadata": {},
   "source": [
    "Here is an example workflow for doing variant effect predictions. For instructions on downloading data, see manuscript repository: [https://github.com/egilfeather/lowrank-s2f-code](https://github.com/egilfeather/lowrank-s2f-code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb1b362",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_path = \"./data/MPRA_eQTL.vcf\"\n",
    "df = pd.read_csv(\"./data/MPRA_eQTL.tsv\", sep='\\t', header=0)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4d2a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seillra as sl\n",
    "rank = 256\n",
    "model = sl.Sei_LLRA(k=rank, projection=True, mode = \"variant\", device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f3dabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sei_lora.dataloaders import VariantDataset, VariantDataLoader\n",
    "dataset = VariantDataset(file_path=file_path, fasta_path = \"./resources/hg38_UCSC.fa\", window_size = 4096)\n",
    "dataloader = VariantDataLoader(dataset=dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "model.eval()\n",
    "\n",
    "all_cp_ref = []\n",
    "all_cp_alt = []\n",
    "all_vcf = []\n",
    "\n",
    "progress_bar = tqdm(dataloader, desc=f\"Running {rank} benchmark\")\n",
    "\n",
    "for batch in progress_bar:\n",
    "    ref, alt, vcf = batch\n",
    "\n",
    "\n",
    "    cp_outputs = model((ref, alt))  # both are tuples: (refproj, altproj)\n",
    "\n",
    "\n",
    "    all_cp_ref.append(cp_outputs[0])\n",
    "    all_cp_alt.append(cp_outputs[1])\n",
    "    all_vcf.append(vcf)\n",
    "\n",
    "    # Accumulate by appending to list\n",
    "\n",
    "all_cp_ref = torch.cat([t.detach() for t in all_cp_ref], dim=0).numpy()\n",
    "all_cp_alt = torch.cat([t.detach() for t in all_cp_alt], dim=0).numpy()\n",
    "\n",
    "all_vcf = np.concatenate(all_vcf, axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e99e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "sc_diff = all_cp_alt - all_cp_ref\n",
    "\n",
    "df_pred = pd.DataFrame(all_vcf, columns=[\"CHROM\", \"POS\", \"NAME\", \"REF\", \"ALT\"])\n",
    "df_pred[\"POS\"] = df_pred[\"POS\"].astype(int)\n",
    "\n",
    "seqclass_path = os.path.join( \"./resources/seqclass.names\")\n",
    "with open(seqclass_path, \"r\") as f:\n",
    "    sc_names = []\n",
    "    for line in f:\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) > 1:\n",
    "            sc_names.append(\"-\".join(parts[1:]))\n",
    "        else:\n",
    "            sc_names.append(parts[0])\n",
    "\n",
    "df_sc = pd.DataFrame(sc_diff[:, :40], columns=sc_names[:40])\n",
    "\n",
    "df_pred =  pd.concat([df_pred, df_sc], axis=1)\n",
    "\n",
    "\n",
    "df_ou = df[df['consequence'].isin(['over', 'under'])].copy()\n",
    "df_combine_ou = df_ou.merge(df_pred, left_on = [\"chrom\", \"pos\", \"ref\", \"alt\"], right_on=[\"CHROM\", \"POS\", \"REF\", \"ALT\"], how = \"inner\")\n",
    "binary_labels_ou = (df_combine_ou['consequence'] == 'over')\n",
    "roc_promoter_ou = roc_auc_score(binary_labels_ou, df_combine_ou[\"Promoter\"])\n",
    "print(roc_promoter_ou)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sei_env_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
