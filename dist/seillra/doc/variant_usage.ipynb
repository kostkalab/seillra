{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc6025c4",
   "metadata": {},
   "source": [
    "Import the seillra package, and other helpful packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cc2e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "import seillra as sl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd356337",
   "metadata": {},
   "source": [
    "To load different parts of the models, select a rank, whether the output is chromatin profiles or sequence classes, whether you are predicting for any sequences or specifically variants, and if the model should be on the CPU or GPU. These are selectable when using the `Sei_LLRA` model class, but other functionallity can be customized using the model blocks explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0499ea7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using a MAC\n",
    "if \"qnnpack\" in torch.backends.quantized.supported_engines:\n",
    "    torch.backends.quantized.engine = \"qnnpack\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25a07b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-21 14:58:42,821 - INFO - Checksum verified for url_a6038b62128b5b01_wts: 28a1a49ca62e4d67a62c170df3751f7255db6eea3923455c119c762dde446308\n",
      "2026-01-21 14:58:42,822 - INFO - Loading state dict from /home/ejg66/.cache/seillra/1.5/url_a6038b62128b5b01_wts\n",
      "2026-01-21 14:58:42,879 - INFO - Model weights loaded and set to eval mode.\n",
      "2026-01-21 14:58:44,282 - INFO - Starting download: https://drive.google.com/uc?export=download&id=1DrlkcecVSgj3CH2924Zz8-Oj95Dyry3C -> /home/ejg66/.cache/seillra/1.5/url_9c83e76615711914_wts\n",
      "2026-01-21 14:58:44,282 - INFO - Detected Google Drive URL, using download_from_gdrive for file_id=1DrlkcecVSgj3CH2924Zz8-Oj95Dyry3C\n",
      "2026-01-21 14:58:47,597 - INFO - No virus warning detected. Direct download successful.\n",
      "2026-01-21 14:58:47,613 - INFO - Successfully downloaded to /home/ejg66/.cache/seillra/1.5/url_9c83e76615711914_wts.part\n",
      "2026-01-21 14:58:47,616 - INFO - Download complete: /home/ejg66/.cache/seillra/1.5/url_9c83e76615711914_wts\n",
      "2026-01-21 14:58:47,665 - INFO - Checksum verified for url_9c83e76615711914_wts: ce0baa7e8533604ab579a37ada184832c716e61b285a04b2f97db9367b351df7\n",
      "2026-01-21 14:58:47,665 - INFO - Weight file ready: /home/ejg66/.cache/seillra/1.5/url_9c83e76615711914_wts\n",
      "2026-01-21 14:58:47,666 - INFO - Loading state dict from /home/ejg66/.cache/seillra/1.5/url_9c83e76615711914_wts\n",
      "2026-01-21 14:58:47,677 - INFO - Model weights loaded and set to eval mode.\n"
     ]
    }
   ],
   "source": [
    "rank = 256 # 1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048\n",
    "sequence_classes = False # True\n",
    "sequence_type = \"sequence\" # \"variant\"\n",
    "quant = \"CPU\" # \"GPU\", \"GPU_fp16\", \"GPU_int8\"\n",
    "model = sl.Sei_LLRA(k=rank, projection=sequence_classes, mode = sequence_type, quant=quant)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37098f31",
   "metadata": {},
   "source": [
    "This can be done using `torch.nn.Sequential` as well. For an un-quantized (GPU) model see the commented code. Note that one should be careful about handeling forward and reverse-complement sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1563668",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'seillra' has no attribute 'get_sei_trunk_q'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m mod_trunk \u001b[38;5;241m=\u001b[39m \u001b[43msl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_sei_trunk_q\u001b[49m() \u001b[38;5;66;03m# sm.get_sei_trunk().load_weights()\u001b[39;00m\n\u001b[1;32m      2\u001b[0m mod_head  \u001b[38;5;241m=\u001b[39m sl\u001b[38;5;241m.\u001b[39mget_sei_head_llra_q(k\u001b[38;5;241m=\u001b[39mrank) \u001b[38;5;66;03m# sl.get_sei_head_llra(k=rank)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m mod_projection \u001b[38;5;241m=\u001b[39m sm\u001b[38;5;241m.\u001b[39mget_sei_projection()\u001b[38;5;241m.\u001b[39mload_weights()\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'seillra' has no attribute 'get_sei_trunk_q'"
     ]
    }
   ],
   "source": [
    "mod_trunk = sl.get_sei_trunk(quant = quant) # sm.get_sei_trunk().load_weights()\n",
    "mod_head  = sl.get_sei_head_llra(k=rank, quant = quant) # sl.get_sei_head_llra(k=rank)\n",
    "mod_projection = sl.get_sei_projection(quant = quant)\n",
    "mod_projection.set_mode(sequence_type)\n",
    "\n",
    "#- Make a full model\n",
    "mod = torch.nn.Sequential(collections.OrderedDict([\n",
    "    ('trunk', mod_trunk),\n",
    "    ('head', mod_head),\n",
    "    ('projection', mod_projection)\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d344af6",
   "metadata": {},
   "source": [
    "Here is an example of running the model on a random one-hot encoded sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a7dc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = torch.randint(0, 4, (16, 4096))\n",
    "x = torch.nn.functional.one_hot(sequences, num_classes=4).permute(0, 2, 1).float()\n",
    "print(x.shape)\n",
    "# - run the model\n",
    "out = model(x.to(\"cpu\"))\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd4d5a8",
   "metadata": {},
   "source": [
    "Now we can try a different rank, use the sequenc classes and use the GPU. Note that this will remain on the CPU if you do not have access to a cuda enabled GPU (Apple M-series GPUs are not cuda enabled)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e40de23",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = 1 # 1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048\n",
    "sequence_classes = True # True\n",
    "sequence_type = \"sequence\" # \"variant\"\n",
    "quant = \"GPU\" # \"GPU\", \"GPU_fp16\", \"GPU_int8\"\n",
    "model2 = sl.Sei_LLRA(k=rank, projection=sequence_classes, mode = sequence_type, quant=quant)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c7ead4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = torch.randint(0, 4, (16, 4096))\n",
    "x = torch.nn.functional.one_hot(sequences, num_classes=4).permute(0, 2, 1).float()\n",
    "print(x.shape)\n",
    "# - run the model\n",
    "out = model2(x.to(\"cuda\"))\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c5207f",
   "metadata": {},
   "source": [
    "This can also be done for getting predictions for variants. We will set up example reference and alternate allele sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101af095",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = 1 # 1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048\n",
    "sequence_classes = True # False\n",
    "sequence_type = \"variant\" # \"sequence\"\n",
    "device = \"cpu\" # \"cuda\", \"cuda:0\"\n",
    "model3 = sl.Sei_LLRA(k=rank, projection=sequence_classes, mode = sequence_type, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac87a6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_sequences = torch.randint(0, 4, (16, 4096))\n",
    "alt_sequences = ref_sequences.clone()\n",
    "center_idx = 4096 // 2\n",
    "alt_sequences[:, center_idx] = (ref_sequences[:, center_idx] + 1) % 4\n",
    "\n",
    "\n",
    "x_ref = torch.nn.functional.one_hot(ref_sequences, num_classes=4).permute(0, 2, 1).float()\n",
    "x_alt = torch.nn.functional.one_hot(alt_sequences, num_classes=4).permute(0, 2, 1).float()\n",
    "\n",
    "print(x_ref.shape, x_alt.shape)\n",
    "input = (x_ref, x_alt)\n",
    "out_ref, out_alt = model3(input)\n",
    "print(out_ref.shape, out_alt.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a90917d",
   "metadata": {},
   "source": [
    "Here is an example workflow for doing variant effect predictions. For instructions on downloading data, see manuscript repository: [https://github.com/egilfeather/lowrank-s2f-code](https://github.com/egilfeather/lowrank-s2f-code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb1b362",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_path = \"./data/MPRA_eQTL.vcf\"\n",
    "df = pd.read_csv(\"./data/MPRA_eQTL.tsv\", sep='\\t', header=0)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4d2a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seillra as sl\n",
    "rank = 256\n",
    "model = sl.Sei_LLRA(k=rank, projection=True, mode = \"variant\", quant=\"CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ae5314",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "# from pyfaidx import Fasta\n",
    "# from torch.amp import autocast, GradScaler\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import random\n",
    "import h5py\n",
    "import os\n",
    "\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from pybedtools import BedTool\n",
    "from itertools import islice\n",
    "import math\n",
    "\n",
    "LOOKUP = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n",
    "\n",
    "class VariantDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, file_path, fasta_path = \"\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            bed_path (str): Path to the BED file with positions.\n",
    "            scores_path (str): Path to the file with 61-dimensional scores.\n",
    "            fasta_path (str): Path to the FASTA file for sequence retrieval.\n",
    "        \"\"\"\n",
    "        self.genome = self._load_fasta(fasta_path)\n",
    "        self.vcf_positions = pd.read_csv(file_path, comment=\"#\", sep=\"\\t\", header = None)\n",
    "        self.vcf_positions.columns = [\"CHROM\", \"POS\", \"STRAND\", \"REF\", \"ALT\" ]\n",
    "        self.vcf_positions[\"CHROM\"] = self.vcf_positions[\"CHROM\"].apply(\n",
    "            lambda x: f\"chr{x}\" if not str(x).startswith(\"chr\") else x\n",
    "        )\n",
    "        self.window_size = 4096\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vcf_positions)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "    \n",
    "        vcf_row = self.vcf_positions.iloc[index]\n",
    "        chrom, pos, strand, ref, alt = vcf_row\n",
    "\n",
    "        center = pos + (len(ref) // 2)\n",
    "        start = center - (self.window_size // 2) - 1\n",
    "        end = start + self.window_size\n",
    "\n",
    "        sequence = self._get_sequence(chrom, start, end)\n",
    "        # Find where REF should be\n",
    "        ref_start = (self.window_size // 2) - (len(ref) // 2)\n",
    "        ref_end = ref_start + len(ref)\n",
    "        ref_seq_segment = sequence[ref_start:ref_end]\n",
    "\n",
    "        # Skip if reference doesn't match\n",
    "        if ref_seq_segment != ref:\n",
    "            if ref_seq_segment != alt:\n",
    "                return None, None, None\n",
    "            else:\n",
    "                temp = sequence\n",
    "                sequence = temp[:ref_start] + ref + temp[ref_end:]\n",
    "\n",
    "        alt_sequence = sequence[:ref_start] + alt + sequence[ref_end:]\n",
    "        if len(alt_sequence) < self.window_size:\n",
    "            alt_sequence += \"N\" * (self.window_size - len(alt_sequence))\n",
    "        elif len(alt_sequence) > self.window_size:\n",
    "            alt_sequence = alt_sequence[:self.window_size]\n",
    "\n",
    "        row = vcf_row.to_numpy().tolist()\n",
    "        return torch.tensor(self.returnonehot(sequence, index=index), dtype=torch.float32), torch.tensor(self.returnonehot(alt_sequence, index=index), dtype=torch.float32), row\n",
    "\n",
    "\n",
    "    def _load_fasta(self, fasta_path):\n",
    "        \"\"\"\n",
    "        Load a FASTA file into a dictionary for fast sequence retrieval.\n",
    "        \"\"\"\n",
    "        genome = {}\n",
    "        for record in SeqIO.parse(fasta_path, \"fasta\"):\n",
    "            genome[record.id] = str(record.seq)\n",
    "        return genome\n",
    "\n",
    "    def _get_sequence(self, chrom, start, end, strand = \"+\"):\n",
    "        \"\"\"\n",
    "        Retrieve a sequence from the FASTA dictionary based on chromosome and positions.\n",
    "        \"\"\"\n",
    "        seq = []\n",
    "        # Left pad if start < 0\n",
    "        if start < 0:\n",
    "            seq.append(\"N\" * (-start))\n",
    "            start = 0\n",
    "        # Middle\n",
    "        if start < len(self.genome[chrom]):\n",
    "            seq.append(self.genome[chrom][start:min(end, len(self.genome[chrom]))])\n",
    "        # Right pad if end past chrom length\n",
    "        if end > len(self.genome[chrom]):\n",
    "            seq.append(\"N\" * (end - len(self.genome[chrom])))\n",
    "        return \"\".join(seq).upper()\n",
    "    \n",
    "    def _insert_allele(self, chrom, start, end, ref, alt, strand = \"+\"):\n",
    "        \"\"\"\n",
    "        Replace the REF allele at the center of the sequence with ALT.\n",
    "        \n",
    "        Args:\n",
    "            sequence (str): the reference sequence window\n",
    "            ref (str): the reference allele (from VCF)\n",
    "            alt (str): the alternate allele (from VCF)\n",
    "\n",
    "        Returns:\n",
    "            str: the modified sequence with the allele inserted\n",
    "        \"\"\"\n",
    "        for char in ref:\n",
    "            if char not in LOOKUP:\n",
    "                return None\n",
    "        for char in alt:\n",
    "            if char not in LOOKUP:\n",
    "                return None\n",
    "        sequence = self._get_sequence(chrom, start, end, strand)\n",
    "        sequence = sequence.upper()\n",
    "        center_idx = self.window_size//2\n",
    "        ref_len = len(ref)\n",
    "        alt_len = len(alt)\n",
    "        diff = ref_len - alt_len\n",
    "\n",
    "        start_idx = center_idx -1 - ref_len//2\n",
    "        end_idx = start_idx + (ref_len +1)//2\n",
    "\n",
    "        ref_seq_segment = sequence[start_idx:end_idx]\n",
    "\n",
    "        if ref_seq_segment != ref:\n",
    "            return None\n",
    "        new_sequence = sequence[:start_idx] + alt + sequence[end_idx:]\n",
    "        if diff >=0:\n",
    "            new_sequence = self._get_sequence(chrom, start-(diff//2), start, strand) + new_sequence + self._get_sequence(chrom, end, end+((diff+1)//2), strand)\n",
    "        else:\n",
    "            new_sequence = new_sequence[(-diff//2):-(-diff+1)//2]\n",
    "\n",
    "        return new_sequence.upper()  \n",
    "\n",
    "    def returnonehot(self, string):\n",
    "        \"\"\"\n",
    "        One-hot encode a DNA sequence.\n",
    "        \n",
    "        Args:\n",
    "            string (str): DNA sequence.\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: One-hot encoded matrix of shape (4, len(sequence)).\n",
    "        \"\"\"\n",
    "        string = string.upper()\n",
    "        lookup = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n",
    "        tmp = np.array(list(string))\n",
    "        icol = np.where(tmp != 'N')[0]\n",
    "        out = np.zeros((4, len(tmp)), dtype=np.float32)\n",
    "        irow = np.array([lookup[i] for i in tmp[icol]])\n",
    "\n",
    "        if len(icol) > 0:\n",
    "            out[irow, icol] = 1\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class SeqDataLoader(DataLoader):\n",
    "    def __init__(self, dataset, *, batch_size=1, n_samples=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Custom DataLoader that limits the number of batches per epoch.\n",
    "\n",
    "        Args:\n",
    "            n_batches (int): Maximum number of batches to yield per epoch.\n",
    "        \"\"\"\n",
    "        self.user_n_samples = n_samples  # Save the user's request\n",
    "        self.batch_size_here = batch_size\n",
    "        print(self.batch_size_here)\n",
    "        self.__pl_cls_kwargs__ = {\n",
    "            \"dataset\": dataset,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"n_samples\": n_samples,\n",
    "            **kwargs\n",
    "        }\n",
    "        if getattr(dataset, \"mode\", None) == \"variant_prediction\":\n",
    "            kwargs[\"collate_fn\"] = safe_collate\n",
    "\n",
    "        super().__init__(dataset=dataset, batch_size=batch_size, **kwargs)\n",
    "\n",
    "        dataset_size = len(self.dataset)\n",
    "        print(\"Batch size after DataLoader:\", self.batch_size_here)\n",
    "        print(\"Batch size after DataLoader:\", self.batch_size)\n",
    "        print(dataset_size)\n",
    "        # print(batch_size)\n",
    "        max_batches = math.ceil(int(dataset_size) / int(self.batch_size_here))\n",
    "        print(max_batches)\n",
    "        if self.user_n_samples is not None:\n",
    "            user_n_batches = math.ceil(self.user_n_samples / self.batch_size_here)\n",
    "            self._effective_batches = min(user_n_batches, max_batches)\n",
    "        else:\n",
    "            self._effective_batches = max_batches\n",
    "        print(self._effective_batches )\n",
    "\n",
    "    def __iter__(self):\n",
    "        base_iter = super().__iter__()\n",
    "        return islice(base_iter, self._effective_batches)\n",
    "        \n",
    "    def __len__(self):\n",
    "            return self._effective_batches \n",
    "\n",
    "def safe_collate(batch):\n",
    "    # batch is a list of (x, y, v)\n",
    "    batch = [item for item in batch if item[0] is not None]\n",
    "    if not batch:\n",
    "        print(\"Not Batch\")\n",
    "        return None  # whole batch is invalid\n",
    "  \n",
    "    xs, ys, vs = zip(*batch)\n",
    "    return torch.stack(xs),torch.stack(ys), np.stack(vs)\n",
    "\n",
    "class VariantDataLoader(SeqDataLoader):\n",
    "    def __init__(self, dataset, *, batch_size=1, n_samples = None, **kwargs):\n",
    "        \"\"\"\n",
    "        Custom DataLoader that limits the number of batches per epoch.\n",
    "\n",
    "        Args:\n",
    "            n_batches (int): Maximum number of batches to yield per epoch.\n",
    "        \"\"\"\n",
    "\n",
    "        kwargs.pop(\"collate_fn\", None)\n",
    "        super().__init__(dataset = dataset, batch_size=batch_size, n_samples = n_samples, collate_fn = safe_collate, **kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f3dabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sei_lora.dataloaders import VariantDataset, VariantDataLoader\n",
    "dataset = VariantDataset(file_path=file_path, fasta_path = \"./resources/hg38_UCSC.fa\", window_size = 4096)\n",
    "dataloader = VariantDataLoader(dataset=dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "model.eval()\n",
    "\n",
    "all_cp_ref = []\n",
    "all_cp_alt = []\n",
    "all_vcf = []\n",
    "\n",
    "progress_bar = tqdm(dataloader, desc=f\"Running {rank} benchmark\")\n",
    "\n",
    "for batch in progress_bar:\n",
    "    ref, alt, vcf = batch\n",
    "\n",
    "\n",
    "    cp_outputs = model((ref, alt))  # both are tuples: (refproj, altproj)\n",
    "\n",
    "\n",
    "    all_cp_ref.append(cp_outputs[0])\n",
    "    all_cp_alt.append(cp_outputs[1])\n",
    "    all_vcf.append(vcf)\n",
    "\n",
    "    # Accumulate by appending to list\n",
    "\n",
    "all_cp_ref = torch.cat([t.detach() for t in all_cp_ref], dim=0).numpy()\n",
    "all_cp_alt = torch.cat([t.detach() for t in all_cp_alt], dim=0).numpy()\n",
    "\n",
    "all_vcf = np.concatenate(all_vcf, axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e99e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "sc_diff = all_cp_alt - all_cp_ref\n",
    "\n",
    "df_pred = pd.DataFrame(all_vcf, columns=[\"CHROM\", \"POS\", \"NAME\", \"REF\", \"ALT\"])\n",
    "df_pred[\"POS\"] = df_pred[\"POS\"].astype(int)\n",
    "\n",
    "seqclass_path = os.path.join( \"./resources/seqclass.names\")\n",
    "with open(seqclass_path, \"r\") as f:\n",
    "    sc_names = []\n",
    "    for line in f:\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) > 1:\n",
    "            sc_names.append(\"-\".join(parts[1:]))\n",
    "        else:\n",
    "            sc_names.append(parts[0])\n",
    "\n",
    "df_sc = pd.DataFrame(sc_diff[:, :40], columns=sc_names[:40])\n",
    "\n",
    "df_pred =  pd.concat([df_pred, df_sc], axis=1)\n",
    "\n",
    "\n",
    "df_ou = df[df['consequence'].isin(['over', 'under'])].copy()\n",
    "df_combine_ou = df_ou.merge(df_pred, left_on = [\"chrom\", \"pos\", \"ref\", \"alt\"], right_on=[\"CHROM\", \"POS\", \"REF\", \"ALT\"], how = \"inner\")\n",
    "binary_labels_ou = (df_combine_ou['consequence'] == 'over')\n",
    "roc_promoter_ou = roc_auc_score(binary_labels_ou, df_combine_ou[\"Promoter\"])\n",
    "print(roc_promoter_ou)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sei_env_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
