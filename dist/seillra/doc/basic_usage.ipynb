{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc6025c4",
   "metadata": {},
   "source": [
    "**STEP 1:** \n",
    "\n",
    "Import `seillra` and other packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56cc2e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "import seillra as sl\n",
    "import seimodel as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd356337",
   "metadata": {},
   "source": [
    "**STEP 2:**\n",
    "\n",
    "- Decide on rank of approximate linear layers: 64\n",
    "- Decide on the output type: chromatin profiles (d=231,907) or sequence classes (d=61)\n",
    "- Decide on the device to use for inference (CPU)\n",
    "- If infierence is on arm64 hardware (e.g. macbook), use `qnnpack` as backend if `quant == \"CPU\"`\n",
    "\n",
    "Then load the according model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25a07b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using quantized engine: fbgemm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-20 14:52:15,266 - INFO - Checksum verified for url_a6038b62128b5b01_wts: 28a1a49ca62e4d67a62c170df3751f7255db6eea3923455c119c762dde446308\n",
      "2026-01-20 14:52:15,267 - INFO - Loading state dict from /home/kostka/.cache/seillra/1.3/url_a6038b62128b5b01_wts\n",
      "2026-01-20 14:52:15,515 - INFO - Model weights loaded and set to eval mode.\n",
      "2026-01-20 14:52:17,332 - INFO - Starting download: https://drive.google.com/uc?export=download&id=1xbioJZnY9p-2xp0QlLrgYxqVf16epupC -> /home/kostka/.cache/seillra/1.3/url_b9a0eb4608886e0d_wts\n",
      "2026-01-20 14:52:17,332 - INFO - Detected Google Drive URL, using download_from_gdrive for file_id=1xbioJZnY9p-2xp0QlLrgYxqVf16epupC\n",
      "2026-01-20 14:52:19,212 - INFO - No virus warning detected. Direct download successful.\n",
      "2026-01-20 14:52:19,216 - INFO - Successfully downloaded to /home/kostka/.cache/seillra/1.3/url_b9a0eb4608886e0d_wts.part\n",
      "2026-01-20 14:52:19,219 - INFO - Download complete: /home/kostka/.cache/seillra/1.3/url_b9a0eb4608886e0d_wts\n",
      "2026-01-20 14:52:19,234 - INFO - Checksum verified for url_b9a0eb4608886e0d_wts: e50fe852023657a60952698c591b291df0e4db3111a0e8ea122546f70eee27c3\n",
      "2026-01-20 14:52:19,234 - INFO - Weight file ready: /home/kostka/.cache/seillra/1.3/url_b9a0eb4608886e0d_wts\n",
      "2026-01-20 14:52:19,235 - INFO - Loading state dict from /home/kostka/.cache/seillra/1.3/url_b9a0eb4608886e0d_wts\n",
      "2026-01-20 14:52:19,332 - INFO - Model weights loaded and set to eval mode.\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "\n",
    "# - what model to load\n",
    "rank = 64\n",
    "sequence_classes = False  # True\n",
    "sequence_type = \"sequence\"  # \"variant\"\n",
    "quant = \"CPU\"  # None, \"GPU_fp16\", \"GPU_int8\"\n",
    "\n",
    "# - CPU quantization backend\n",
    "if quant == \"CPU\":\n",
    "    if platform.machine() == \"arm64\":\n",
    "        if \"qnnpack\" in torch.backends.quantized.supported_engines:\n",
    "            torch.backends.quantized.engine = \"qnnpack\"\n",
    "        else:\n",
    "            raise RuntimeError(\"QNNPACK not supported on this arm64 machine.\")\n",
    "    if platform.machine() == \"x86_64\":\n",
    "        if \"fbgemm\" in torch.backends.quantized.supported_engines:\n",
    "            torch.backends.quantized.engine = \"fbgemm\"\n",
    "        else:\n",
    "            raise RuntimeError(\"FBGEMM not supported on this x86_64 machine.\")\n",
    "    print(\"Using quantized engine:\", torch.backends.quantized.engine)\n",
    "\n",
    "# - load the model\n",
    "model = sl.Sei_LLRA(\n",
    "    k=rank, projection=sequence_classes, mode=sequence_type, quant=quant\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f933c1d0",
   "metadata": {},
   "source": [
    "(See the readme file for loading the Sei model components (trunk, head, projection) seperately)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af1b13f",
   "metadata": {},
   "source": [
    "**STEP 3:**\n",
    "\n",
    "Use the model to predict sequence classes (since we indlucded the projection model for DNA input sequences):\n",
    "\n",
    "- Generate 16 random input sequences\n",
    "- Apply the `seillra` model\n",
    "- Print (part of) the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8b5e15",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m x = torch.nn.functional.one_hot(sequences, num_classes=\u001b[32m4\u001b[39m).permute(\u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m).float()\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# - run the model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m out = \u001b[43mmodel\u001b[49m(x.to(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# - typically discard the \"unintersting\" sequence classes (see the original Sei repository/paper)\u001b[39;00m\n\u001b[32m      9\u001b[39m res = out[:, :\u001b[32m40\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# - random 1-hot sequences\n",
    "sequences = torch.randint(0, 4, (16, 4096))\n",
    "x = torch.nn.functional.one_hot(sequences, num_classes=4).permute(0, 2, 1).float()\n",
    "\n",
    "# - run the model\n",
    "out = model(x.to(\"cpu\"))\n",
    "\n",
    "# - typically discard the \"unintersting\" sequence classes (see the original Sei repository/paper)\n",
    "res = out[:, :40]\n",
    "\n",
    "# - what do we have?\n",
    "print(res[:3, :11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc6f0eaa",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchview'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchview\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m draw_graph\n\u001b[32m      3\u001b[39m graph = draw_graph(model, input_size=(\u001b[32m1\u001b[39m, \u001b[32m4\u001b[39m, \u001b[32m4096\u001b[39m))\n\u001b[32m      4\u001b[39m graph.visual_graph\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torchview'"
     ]
    }
   ],
   "source": [
    "from torchview import draw_graph\n",
    "\n",
    "graph = draw_graph(model, input_size=(1, 4, 4096))\n",
    "graph.visual_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37098f31",
   "metadata": {},
   "source": [
    "This can be done using `torch.nn.Sequential` as well. For an un-quantized (GPU) model see the commented code. Note that one should be careful about handeling forward and reverse-complement sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1563668",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_trunk = sl.get_sei_trunk_q()  # sm.get_sei_trunk().load_weights()\n",
    "mod_head = sl.get_sei_head_llra_q(k=rank)  # sl.get_sei_head_llra(k=rank)\n",
    "mod_projection = sm.get_sei_projection().load_weights()\n",
    "mod_projection.set_mode(sequence_type)\n",
    "\n",
    "# - Make a full model\n",
    "mod = torch.nn.Sequential(\n",
    "    collections.OrderedDict(\n",
    "        [(\"trunk\", mod_trunk), (\"head\", mod_head), (\"projection\", mod_projection)]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d344af6",
   "metadata": {},
   "source": [
    "Here is an example of running the model on a random one-hot encoded sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a7dc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = torch.randint(0, 4, (16, 4096))\n",
    "x = torch.nn.functional.one_hot(sequences, num_classes=4).permute(0, 2, 1).float()\n",
    "print(x.shape)\n",
    "# - run the model\n",
    "out = model(x.to(\"cpu\"))\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd4d5a8",
   "metadata": {},
   "source": [
    "Now we can try a different rank, use the sequenc classes and use the GPU. Note that this will remain on the CPU if you do not have access to a cuda enabled GPU (Apple M-series GPUs are not cuda enabled)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e40de23",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = 1  # 1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048\n",
    "sequence_classes = True  # True\n",
    "sequence_type = \"sequence\"  # \"variant\"\n",
    "device = \"cuda\"  # \"cpu\", \"cuda:0\"\n",
    "model2 = sl.Sei_LLRA(\n",
    "    k=rank, projection=sequence_classes, mode=sequence_type, device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c7ead4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = torch.randint(0, 4, (16, 4096))\n",
    "x = torch.nn.functional.one_hot(sequences, num_classes=4).permute(0, 2, 1).float()\n",
    "print(x.shape)\n",
    "# - run the model\n",
    "out = model2(x.to(\"cuda\"))\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c5207f",
   "metadata": {},
   "source": [
    "This can also be done for getting predictions for variants. We will set up example reference and alternate allele sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101af095",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = 1  # 1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048\n",
    "sequence_classes = True  # False\n",
    "sequence_type = \"variant\"  # \"sequence\"\n",
    "device = \"cpu\"  # \"cuda\", \"cuda:0\"\n",
    "model3 = sl.Sei_LLRA(\n",
    "    k=rank, projection=sequence_classes, mode=sequence_type, device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac87a6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_sequences = torch.randint(0, 4, (16, 4096))\n",
    "alt_sequences = ref_sequences.clone()\n",
    "center_idx = 4096 // 2\n",
    "alt_sequences[:, center_idx] = (ref_sequences[:, center_idx] + 1) % 4\n",
    "\n",
    "\n",
    "x_ref = (\n",
    "    torch.nn.functional.one_hot(ref_sequences, num_classes=4).permute(0, 2, 1).float()\n",
    ")\n",
    "x_alt = (\n",
    "    torch.nn.functional.one_hot(alt_sequences, num_classes=4).permute(0, 2, 1).float()\n",
    ")\n",
    "\n",
    "print(x_ref.shape, x_alt.shape)\n",
    "input = (x_ref, x_alt)\n",
    "out_ref, out_alt = model3(input)\n",
    "print(out_ref.shape, out_alt.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a90917d",
   "metadata": {},
   "source": [
    "Here is an example workflow for doing variant effect predictions. For instructions on downloading data, see manuscript repository: [https://github.com/egilfeather/lowrank-s2f-code](https://github.com/egilfeather/lowrank-s2f-code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb1b362",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./data/MPRA_eQTL.vcf\"\n",
    "df = pd.read_csv(\"./data/MPRA_eQTL.tsv\", sep=\"\\t\", header=0)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4d2a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seillra as sl\n",
    "\n",
    "rank = 256\n",
    "model = sl.Sei_LLRA(k=rank, projection=True, mode=\"variant\", device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f3dabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sei_lora.dataloaders import VariantDataset, VariantDataLoader\n",
    "\n",
    "dataset = VariantDataset(\n",
    "    file_path=file_path, fasta_path=\"./resources/hg38_UCSC.fa\", window_size=4096\n",
    ")\n",
    "dataloader = VariantDataLoader(\n",
    "    dataset=dataset, batch_size=32, shuffle=False, num_workers=0\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "all_cp_ref = []\n",
    "all_cp_alt = []\n",
    "all_vcf = []\n",
    "\n",
    "progress_bar = tqdm(dataloader, desc=f\"Running {rank} benchmark\")\n",
    "\n",
    "for batch in progress_bar:\n",
    "    ref, alt, vcf = batch\n",
    "\n",
    "    cp_outputs = model((ref, alt))  # both are tuples: (refproj, altproj)\n",
    "\n",
    "    all_cp_ref.append(cp_outputs[0])\n",
    "    all_cp_alt.append(cp_outputs[1])\n",
    "    all_vcf.append(vcf)\n",
    "\n",
    "    # Accumulate by appending to list\n",
    "\n",
    "all_cp_ref = torch.cat([t.detach() for t in all_cp_ref], dim=0).numpy()\n",
    "all_cp_alt = torch.cat([t.detach() for t in all_cp_alt], dim=0).numpy()\n",
    "\n",
    "all_vcf = np.concatenate(all_vcf, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e99e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "sc_diff = all_cp_alt - all_cp_ref\n",
    "\n",
    "df_pred = pd.DataFrame(all_vcf, columns=[\"CHROM\", \"POS\", \"NAME\", \"REF\", \"ALT\"])\n",
    "df_pred[\"POS\"] = df_pred[\"POS\"].astype(int)\n",
    "\n",
    "seqclass_path = os.path.join(\"./resources/seqclass.names\")\n",
    "with open(seqclass_path, \"r\") as f:\n",
    "    sc_names = []\n",
    "    for line in f:\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) > 1:\n",
    "            sc_names.append(\"-\".join(parts[1:]))\n",
    "        else:\n",
    "            sc_names.append(parts[0])\n",
    "\n",
    "df_sc = pd.DataFrame(sc_diff[:, :40], columns=sc_names[:40])\n",
    "\n",
    "df_pred = pd.concat([df_pred, df_sc], axis=1)\n",
    "\n",
    "\n",
    "df_ou = df[df[\"consequence\"].isin([\"over\", \"under\"])].copy()\n",
    "df_combine_ou = df_ou.merge(\n",
    "    df_pred,\n",
    "    left_on=[\"chrom\", \"pos\", \"ref\", \"alt\"],\n",
    "    right_on=[\"CHROM\", \"POS\", \"REF\", \"ALT\"],\n",
    "    how=\"inner\",\n",
    ")\n",
    "binary_labels_ou = df_combine_ou[\"consequence\"] == \"over\"\n",
    "roc_promoter_ou = roc_auc_score(binary_labels_ou, df_combine_ou[\"Promoter\"])\n",
    "print(roc_promoter_ou)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seillra_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
