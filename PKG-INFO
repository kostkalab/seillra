Metadata-Version: 2.4
Name: seillra
Version: 1.4.2
Summary: Easy access to a low-rank approximation of Sei
Description-Content-Type: text/markdown
License-File: LICENSE.txt
Requires-Dist: torch
Requires-Dist: numpy
Requires-Dist: scipy
Requires-Dist: requests
Requires-Dist: platformdirs
Requires-Dist: tqdm
Requires-Dist: pyyaml
Requires-Dist: seimodel@ git+https://github.com/kostkalab/seimodel.git
Dynamic: license-file


## Introduction

This package exposes the Sei trunk–head–projection pipeline where the head is replaced with post-hoc low-rank linear layers, and makes both full-precision and quantized (CPU/GPU-friendly) variants of each sub-model available. Refer to the [Sei-LLRA preprint](tbd) for architectural and training details.




## Installation

Install from github with:

```bash
$ pip install git+https://github.com/kostkalab/seillra.git
```

## Usage

Basic usage for creating a quantized Sei model, with rank-64 approximation of linear layers, intended to run on the CPU.
The following demonstrates access to the individual components of Sei:

```python
import torch
import collections
import seillra as sl
import seimodel as sm

# Get component models (trunk, head, projection)
mod_trunk = sl.get_trunk(quant='CPU')
mod_head  = sl.get_head_llra(k=16, quant='CPU')
mod_projection = sl.get_projection(quant='CPU')


#- Constuct the combeined model from its parts:
mod = torch.nn.Sequential(collections.OrderedDict([
    ('trunk', mod_trunk),
    ('head', mod_head),
    ('projection', mod_projection)
]))
```

Alternatively, there is a convenience function:


```python
mod = sl.Sei_LLRA(k=64, projection=True, quant = 'CPU')
```

The PyTorch model `mod` is then ready for inference tasks on the CPU.
For GPU-compatible model we provide the following options via the `quant` parameter:

- No quantization: `quant = None`
- CPU quantization: `quant = "CPU"`
- GPU fp16 quantization: `quant = "GPU_fp16"`
- GPU int8 quantization: `quant = "GPU_int8"`

Select from ranks 1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, or None (for full model).

For more detailed examples including applying SeiLLRA to data, please see the following notebooks:

- Chromatin states and sequence classes: [basic_notebook.ipynb](./docs/basic_usage.ipynb)
- Variant effects: [variant_notebook.ipynb](./docs/variant_usage.ipynb)


